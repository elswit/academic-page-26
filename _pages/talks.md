---
layout: page
title: talks
permalink: /talks/
description: Invited talks, conference presentations, and guest lectures
nav: true
nav_order: 5
---

## Invited Talks & Presentations

---

### 2025

<div class="card mt-3">
  <div class="card-body">
    <h5 class="card-title">Advanced Sampling Methods for Machine Learning</h5>
    <h6 class="card-subtitle mb-2 text-muted">Guest Lecture | California State University, Los Angeles</h6>
    <p class="card-text">April 14, 2025 — Los Angeles, CA</p>
  </div>
</div>

---

### 2024

<div class="card mt-3">
  <div class="card-body">
    <h5 class="card-title">Uncertainty Quantification for SciML using Deep Operator Networks</h5>
    <h6 class="card-subtitle mb-2 text-muted">Talk | SIAM Annual Meeting 2024</h6>
    <p class="card-text">March 2024 — Spokane, WA</p>
    <p class="card-text">Part of MS66, a mini-symposium on New Methods in Probabilistic and Science-Guided Machine Learning.</p>
    <p>
      <a href="https://youtu.be/1iTHm9u9lr0?feature=shared" class="btn btn-sm btn-outline-primary"><i class="fab fa-youtube"></i> Video</a>
    </p>
  </div>
</div>

---

### 2023

<div class="card mt-3">
  <div class="card-body">
    <h5 class="card-title">Science-guided Machine Learning for Forward, Inverse, and Control Problems</h5>
    <h6 class="card-subtitle mb-2 text-muted">Invited Talk | Grand Valley State University</h6>
    <p class="card-text">March 2023 — Michigan, USA</p>
    <p class="card-text">Scientific machine learning (SciML) is an interdisciplinary field that solves complex scientific problems by combining computational and algorithmic techniques with machine learning methods. This talk covered the most recent developments in SciML, highlighting limitations of current methodologies and exploring new ideas to address them.</p>
    <p>
      <a href="https://people.cs.vt.edu/sarshar/talks/gvsu/" class="btn btn-sm btn-outline-primary"><i class="fas fa-file-powerpoint"></i> Slides</a>
    </p>
  </div>
</div>

---

### 2022

<div class="card mt-3">
  <div class="card-body">
    <h5 class="card-title">On Time-stepping Methods for Gradient-flow Optimization</h5>
    <h6 class="card-subtitle mb-2 text-muted">Talk | CAIMS/SCMAI 2022</h6>
    <p class="card-text">March 2022 — Ontario, Canada</p>
    <p class="card-text">Gradient-based optimization methods are essential in neural network training in many applications. The evolution of neural network parameters can be considered as an ODE system evolving in pseudo-time towards a local minimum of the objective function. This interpretation allows us to use different time-stepping schemes for the optimization.</p>
    <p>
      <a href="/assets/pdf/adam-gark.pdf" class="btn btn-sm btn-outline-primary"><i class="fas fa-file-powerpoint"></i> Slides</a>
      <a href="https://vtechworks.lib.vt.edu/handle/10919/109996" class="btn btn-sm btn-outline-primary"><i class="fas fa-file-alt"></i> Preprint</a>
    </p>
  </div>
</div>

---

### 2021

<div class="card mt-3">
  <div class="card-body">
    <h5 class="card-title">Linearly Implicit General Linear Methods</h5>
    <h6 class="card-subtitle mb-2 text-muted">Talk | SIAM Conference on Computational Science and Engineering 2021</h6>
    <p class="card-text">March 2021 — Atlanta, GA (Virtual)</p>
    <p class="card-text">Linearly implicit Runge-Kutta methods provide a fitting balance between implicit treatment of stiff systems and computational costs. We extend the class of linearly implicit Runge-Kutta methods to include multi-stage and multi-step methods.</p>
    <p>
      <a href="https://meetings.siam.org/sess/dsp_talk.cfm?p=108262" class="btn btn-sm btn-outline-primary"><i class="fas fa-info-circle"></i> Program Abstract</a>
    </p>
  </div>
</div>

---

### 2019

<div class="card mt-3">
  <div class="card-body">
    <h5 class="card-title">Discrete Multirate GARK Schemes</h5>
    <h6 class="card-subtitle mb-2 text-muted">Talk | SIAM Conference on Computational Science and Engineering 2019</h6>
    <p class="card-text">March 2019 — Spokane, WA</p>
    <p class="card-text">Multirate time integration schemes apply different step sizes to different components of a system based on the local dynamics of the components. This talk focused on high-order multirate methods using the theoretical framework of <em>generalized additive Runge-Kutta (GARK)</em> methods.</p>
    <p>
      <a href="https://arxiv.org/abs/1804.07716" class="btn btn-sm btn-outline-primary"><i class="fas fa-file-alt"></i> Preprint</a>
      <a href="https://epubs.siam.org/doi/abs/10.1137/18M1182875" class="btn btn-sm btn-outline-primary"><i class="fas fa-book"></i> Paper</a>
      <a href="https://www.pathlms.com/siam/courses/10878/sections/14361/video_presentations/127463" class="btn btn-sm btn-outline-primary"><i class="fab fa-youtube"></i> Video</a>
      <a href="https://docs.google.com/presentation/d/1M43xXqBg24S0TZVmhumRr_c_FNAICulXwpaRKIR-eTs/pub?start=false&loop=false&delayms=30000" class="btn btn-sm btn-outline-primary"><i class="fas fa-file-powerpoint"></i> Slides</a>
      <a href="https://meetings.siam.org/sess/dsp_talk.cfm?p=95449" class="btn btn-sm btn-outline-primary"><i class="fas fa-info-circle"></i> Program</a>
    </p>
  </div>
</div>
